{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.011719\ttest-merror:0.127273\n",
      "[1]\ttrain-merror:0.015625\ttest-merror:0.127273\n",
      "[2]\ttrain-merror:0.011719\ttest-merror:0.109091\n",
      "[3]\ttrain-merror:0.007812\ttest-merror:0.081818\n",
      "[4]\ttrain-merror:0.007812\ttest-merror:0.090909\n",
      "pred: [ 3.  1.  3.  3.  3.  1.  0.  1.  1.  4.  5.  5.  5.  1.  3.  1.  0.  0.\n",
      "  0.  0.  0.  1.  1.  3.  3.  1.  0.  0.  1.  1.  1.  2.  2.  2.  2.  0.\n",
      "  0.  0.  0.  4.  4.  4.  4.  4.  2.  2.  2.  3.  0.  0.  3.  3.  3.  0.\n",
      "  0.  0.  2.  2.  2.  2.  2.  0.  0.  0.  0.  3.  3.  0.  0.  3.  2.  2.\n",
      "  1.  0.  0.  3.  3.  4.  4.  0.  0.  4.  4.  2.  0.  4.  4.  5.  5.  3.\n",
      "  1.  5.  5.  5.  0.  0.  0.  4.  4.  0.  0.  0.  0.  1.  1.  3.  3.  2.\n",
      "  2.  0.]\n",
      "predicting, classification error=0.090909\n",
      "0.909090909091\n",
      "[0]\ttrain-merror:0.011719\ttest-merror:0.127273\n",
      "[1]\ttrain-merror:0.015625\ttest-merror:0.127273\n",
      "[2]\ttrain-merror:0.011719\ttest-merror:0.109091\n",
      "[3]\ttrain-merror:0.007812\ttest-merror:0.081818\n",
      "[4]\ttrain-merror:0.007812\ttest-merror:0.090909\n",
      "('yprob::', (110, 6))\n",
      "prob: [ 0.12211822  0.12527415  0.12204564  0.38662398  0.12199699  0.12194106]\n",
      "ylabel: [3 1 3 3 3 1 0 1 1 4 5 5 5 1 3 1 0 0 0 0 0 1 1 3 3 1 0 0 1 1 1 2 2 2 2 0 0\n",
      " 0 0 4 4 4 4 4 2 2 2 3 0 0 3 3 3 0 0 0 2 2 2 2 2 0 0 0 0 3 3 0 0 3 2 2 1 0\n",
      " 0 3 3 4 4 0 0 4 4 2 0 4 4 5 5 3 1 5 5 5 0 0 0 4 4 0 0 0 0 1 1 3 3 2 2 0]\n",
      "predicting, classification error=0.090909\n"
     ]
    }
   ],
   "source": [
    "# label need to be 0 to num_class -1\n",
    "data = np.loadtxt('./data/dermatology.data', delimiter=',',converters={33: lambda x:int(x == '?'), 34: lambda x:int(x)-1 } )\n",
    "sz = data.shape\n",
    "train = data[:int(sz[0] * 0.7), :]\n",
    "test = data[int(sz[0] * 0.7):, :]\n",
    "train_X = train[:,0:33]\n",
    "train_Y = train[:, 34]\n",
    "test_X = test[:,0:33]\n",
    "test_Y = test[:, 34]\n",
    "xg_train = xgb.DMatrix( train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)\n",
    "# setup parameters for xgboost\n",
    "param = {\n",
    "    # use softmax multi-class classification\n",
    "    'objective' : 'multi:softmax',\n",
    "    # scale weight of positive examples\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'silent': 1,\n",
    "    'nthread': 4, \n",
    "    'num_class': 6\n",
    "}\n",
    "\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 5\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "\n",
    "# get prediction\n",
    "pred = bst.predict( xg_test );\n",
    "print (\"pred: %s\" % pred)\n",
    "print ('predicting, classification error=%f' % (sum( int(pred[i]) != test_Y[i] for i in range(len(test_Y))) / float(len(test_Y)) ))\n",
    "print metrics.accuracy_score(test_Y, pred)\n",
    "\n",
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "\n",
    "yprob = bst.predict( xg_test )\n",
    "print (\"yprob::\", yprob.shape)\n",
    "yprob = yprob.reshape( test_Y.shape[0], 6 )\n",
    "ylabel = np.argmax(yprob, axis=1)\n",
    "\n",
    "print (\"prob: %s\" % yprob[0])\n",
    "print (\"ylabel: %s\" % ylabel)\n",
    "print ('predicting, classification error=%f' % (sum( int(ylabel[i]) != test_Y[i] for i in range(len(test_Y))) / float(len(test_Y)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
