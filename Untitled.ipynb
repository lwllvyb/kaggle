{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/column_3C.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-64fa4e272a98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/column_3C.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/column_3C.dat'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Purpose: This script tries to implement a technique called stacking/blending/stacked generalization.\n",
    "The reason I have to make this a runnable script because I found that there isn't really any\n",
    "readable code that demonstrates this technique. You may find the pseudocode in various papers but they\n",
    "are all each kind of different.\n",
    "\n",
    "Author: Eric Chio \"log0\" <im.ckieric@gmail.com>\n",
    "\n",
    "======================================================================================================\n",
    "Summary:\n",
    "\n",
    "Just to test an implementation of stacking. Using a cross-validated random forest and SVMs, I was\n",
    "only able to achieve an accuracy of about 88% (with 1000 trees and up). Using stacked generalization \n",
    "I have seen a maximum of 93.5% accuracy. It does take runs to find it out though. This uses only \n",
    "(10, 20, 10) trees for the three classifiers.\n",
    "\n",
    "This code is heavily inspired from the code shared by Emanuele (https://github.com/emanuele) , but I\n",
    "have cleaned it up to makeit available for easy download and execution.\n",
    "\n",
    "======================================================================================================\n",
    "Methodology:\n",
    "\n",
    "Three classifiers (RandomForestClassifier, ExtraTreesClassifier and a GradientBoostingClassifier\n",
    "are built to be stacked by a LogisticRegression in the end.\n",
    "\n",
    "Some terminologies first, since everyone has their own, I'll define mine to be clear:\n",
    "- DEV SET, this is to be split into the training and validation data. It will be cross-validated.\n",
    "- TEST SET, this is the unseen data to validate the generalization error of our final classifier. This\n",
    "set will never be used to train.\n",
    "\n",
    "======================================================================================================\n",
    "Log Output:\n",
    "\n",
    "X_test.shape = (62L, 6L)\n",
    "blend_train.shape = (247L, 3L)\n",
    "blend_test.shape = (62L, 3L)\n",
    "Training classifier [0]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Training classifier [1]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Training classifier [2]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Y_dev.shape = 247\n",
    "Accuracy = 0.935483870968\n",
    "\n",
    "======================================================================================================\n",
    "Data Set Information:\n",
    "\n",
    "Biomedical data set built by Dr. Henrique da Mota during a medical residence period in the Group\n",
    "of Applied Research in Orthopaedics (GARO) of the Centre MÃ©dico-Chirurgical de RÃ©adaptation des\n",
    "Massues, Lyon, France. The data have been organized in two different but related classification\n",
    "tasks. The first task consists in classifying patients as belonging to one out of three\n",
    "categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150\n",
    "patients). For the second task, the categories Disk Hernia and Spondylolisthesis were merged \n",
    "into a single category labelled as 'abnormal'. Thus, the second task consists in classifying\n",
    "patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 \n",
    "patients). We provide files also for use within the WEKA environment.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Each patient is represented in the data set by six biomechanical attributes derived from the \n",
    "shape and orientation of the pelvis and lumbar spine (in this order): pelvic incidence, pelvic\n",
    "tilt, lumbar lordosis angle, sacral slope, pelvic radius and grade of spondylolisthesis. The\n",
    "following convention is used for the class labels: DH (Disk Hernia), Spondylolisthesis (SL),\n",
    "Normal (NO) and Abnormal (AB).\n",
    "\n",
    "\"\"\"\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "def run(data):\n",
    "    X = np.array([ i[:-1] for i in data ], dtype=float)\n",
    "    Y = np.array([ i[-1] for i in data ])\n",
    "    \n",
    "    # We need to transform the string output to numeric\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(Y)\n",
    "    Y = label_encoder.transform(Y)\n",
    "    \n",
    "    # The DEV SET will be used for all training and validation purposes\n",
    "    # The TEST SET will never be used for training, it is the unseen set.\n",
    "    dev_cutoff = len(Y) * 4/5\n",
    "    X_dev = X[:dev_cutoff]\n",
    "    Y_dev = Y[:dev_cutoff]\n",
    "    X_test = X[dev_cutoff:]\n",
    "    Y_test = Y[dev_cutoff:]\n",
    "    \n",
    "    n_trees = 10\n",
    "    n_folds = 5\n",
    "    \n",
    "    # Our level 0 classifiers\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators = n_trees, criterion = 'gini'),\n",
    "        ExtraTreesClassifier(n_estimators = n_trees * 2, criterion = 'gini'),\n",
    "        GradientBoostingClassifier(n_estimators = n_trees),\n",
    "    ]\n",
    "    \n",
    "    # Ready for cross validation\n",
    "    skf = list(StratifiedKFold(Y_dev, n_folds))\n",
    "    \n",
    "    # Pre-allocate the data\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers\n",
    "    \n",
    "    print 'X_test.shape = %s' % (str(X_test.shape))\n",
    "    print 'blend_train.shape = %s' % (str(blend_train.shape))\n",
    "    print 'blend_test.shape = %s' % (str(blend_test.shape))\n",
    "    \n",
    "    # For each classifier, we train the number of fold times (=len(skf))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print 'Training classifier [%s]' % (j)\n",
    "        blend_test_j = np.zeros((X_test.shape[0], len(skf))) # Number of testing data x Number of folds , we will take the mean of the predictions later\n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            print 'Fold [%s]' % (i)\n",
    "            \n",
    "            # This is the training and validation set\n",
    "            X_train = X_dev[train_index]\n",
    "            Y_train = Y_dev[train_index]\n",
    "            X_cv = X_dev[cv_index]\n",
    "            Y_cv = Y_dev[cv_index]\n",
    "            \n",
    "            clf.fit(X_train, Y_train)\n",
    "            \n",
    "            # This output will be the basis for our blended classifier to train against,\n",
    "            # which is also the output of our classifiers\n",
    "            blend_train[cv_index, j] = clf.predict(X_cv)\n",
    "            blend_test_j[:, i] = clf.predict(X_test)\n",
    "        # Take the mean of the predictions of the cross validation set\n",
    "        blend_test[:, j] = blend_test_j.mean(1)\n",
    "    \n",
    "    print 'Y_dev.shape = %s' % (Y_dev.shape)\n",
    "    \n",
    "    # Start blending!\n",
    "    bclf = LogisticRegression()\n",
    "    bclf.fit(blend_train, Y_dev)\n",
    "    \n",
    "    # Predict now\n",
    "    Y_test_predict = bclf.predict(blend_test)\n",
    "    score = metrics.accuracy_score(Y_test, Y_test_predict)\n",
    "    print 'Accuracy = %s' % (score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_file = 'data/column_3C.dat'\n",
    "\n",
    "    data = [ i for i in csv.reader(file(train_file, 'rb'), delimiter=' ') ]\n",
    "    data = data[1:] # remove header\n",
    "    \n",
    "    best_score = 0.0\n",
    "    \n",
    "    # run many times to get a better result, it's not quite stable.\n",
    "    for i in xrange(1):\n",
    "        print 'Iteration [%s]' % (i)\n",
    "        random.shuffle(data)\n",
    "        score = run(data)\n",
    "        best_score = max(best_score, score)\n",
    "        print\n",
    "        \n",
    "    print 'Best score = %s' % (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
